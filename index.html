<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title> ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark<br></title>
    <link rel="icon" type="image/png" href="assets/arab_logo.png">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
   <link rel="stylesheet" href="style.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    
    <style>
        .button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 10px 18px;
            border-radius: 30px; 
            background-color: #999; /* Dark color */
            color: white;
            font-size: 16px;
            text-decoration: none;
            border: none;
            transition: background-color 0.3s ease;
        }

        .bibtex-block {
          text-align: left;
          white-space: pre-wrap; /* Optional: wraps lines if they're too long */
          background-color: #f8f8f8;
          padding: 1em;
          border-radius: 5px;
          font-family: monospace;
        }

        .ival-logo {
          width:280px;
          height: auto;
        }
        
        .oryx-logo {
          width: 150px;
          height: auto;
        }
        
        .mbzuai-logo {
          width: 480px;
          height: auto;
        }

        .button:hover {
            background-color: #eeeeee; /* Yellow on hover */
        }

        .button .icon {
            margin-right: 8px;
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        .container {
            text-align: center;
        }
        h1, h2 {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container img {
            max-width: 65%; /* Reduced from 80% */
            height: auto;
            margin: 0 auto;
            display: block;
        }
        .fig-container-side {
            text-align: center;
            margin: 0px 0;
        }
        .fig-container-side img:first-child {
            width: 50%;
            margin: 0 2%;
            display: inline-block;
        }
        .fig-container-side img:last-child {
            width: 40%;
            margin: 0 2%;
            display: inline-block;
        }
        p {
            text-align: center;
            max-width: 800px;
            margin: 20px auto;
        }
        ul {
            display: inline-block;
            text-align: left;
            margin: 0 auto;
        }
        .publication-authors {
            text-align: center;
            max-width: 800px;
            margin: 20px auto;
        }
        .citation {
            text-align: left;
            max-width: 600px;
            margin: 20px auto;
            padding: 20px;
        }
        
        .header-container {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 15px; /* Adjust spacing between logo and title */
    }
    
        /* Adjust  size */
        . {
            width: 60px; /* Adjust size as needed */
            height: auto;
            margin: 0;
            display: inline-block;
        }
        
        /* Ensure title aligns properly */
        .title {
            font-size: 3em;
            margin: 0;
            background: linear-gradient(to right, #333333, #666666);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;
            font-family: Geneva, Tahoma, sans-serif;
    }
        .title2 {
            font-size: 2em;
            margin: 0px 0;
            background: linear-gradient(to right, #585858, #666666);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-family:Geneva, Tahoma, sans-serif
        }
         .title3 {
            font-size: 1.5em;
            margin: 0px 0;
            background: linear-gradient(to right, #585858, #666666);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-family:Geneva, Tahoma, sans-serif
        }
        .authors {
           font-size: 18px;
            margin: 20px 0;
        }
        .fig-container {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container img {
            max-width: 60%;
            height: auto;
        }
       .figure-container-d {
          text-align: center;
          display: flex;
          justify-content: center;  /* Centers the image horizontally */
          align-items: center;      /* Centers vertically if needed */
          margin: 30px auto;
          min-height: 300px;        /* Ensures space is reserved */
      }
      
      .figure-container-d img {
          max-width: 60%;
         display: none;  /* Hide all figures initially */
      }

     .figure-container-d img {
          max-width: 60%;
          display: none; /* Hide all figures initially */
           }
      #fig0 {
               display: block; /* Ensure fig0 is visible by default */
           }
     
        .fig-container-s {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container-s img {
            max-width: 40%;
        }
        .caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9em;
        }
        .logo {
            height: 50px;
            width: auto;
            display: inline-block;
            margin: 0;
        }
 /* Footer logos */
        .logos {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px; /* spacing between logos */
            margin: 20px 0;
        } */
        
        .logos img {
            height: 15px;
            width: 10px;
            object-fit: contain;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .citation {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 4px;
            font-family: monospace;
        }
    
       
     .fig-container-side {
            display: flex;
            justify-content: space-between;
            align-items: center;
            text-align: center;
            margin: 30px auto;
            max-width: 900px;
        }
        .fig-container-side img {
            max-width: 45%;
            height: auto;
        }
        .button img {
            height: 20px;
            margin-right: 8px;
        }
    
        .fig-intro {
            text-align: center;
            margin: 30px auto;
            max-width: 900px; /* Ensure the figure and caption share the same width */

        }
        
        .fig-intro img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        
         .fig-caption {
            text-align: center;
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9em;
            max-width: 900px; /* Match figure width */
        }

        /* Style for the abstract section */
        .abstract-section {
            background-color: #eeeeee; /* Light gray color */
            padding: 30px;
            margin: 30px auto;
            max-width: 3000px;
            border-radius: 8px;
        }
        
        .abstract-section p {
            text-align: justify;
            font-size: 16px;
            line-height: 1.6;
            color: #333;
        }
        /* Add space between major sections */
        .section-spacing {
            margin-top: 100px; /* Adjust space between sections */
            margin-bottom: 40px;
        }

        /* Dropdown styling */
        .dropdown-container {
            text-align: center;
            margin: 30px auto;
        }
        .dropdown-container select {
            padding: 10px;
            font-size: 16px;
            border-radius: 5px;
        }
     
     div {
    text-align: center;
   }
     /* Table Styles */
     table {
         width: 80%;
         border-collapse: collapse;
         margin: 20px auto;
         font-size: 14px;
     }
     
     /* Table Headers */
     th {
         background-color: #e2edec;
         color: white;
         border: 1px solid #ddd;
         padding: 10px;
         text-align: center;
     }
     /* Table Rows */
     td {
         padding: 10px;
         border: 1px solid #ddd;
         text-align: center;
     }
     
     /* Alternate Row Colors */
     tbody tr:nth-child(odd) {
         background-color: #f9f9f9;
     }
     
     /* Add hover effect */
     tbody tr:hover {
         background-color: #e8e8e8;
     }
     
     /* Style for the Medal Icon üèÖ */
     b {
         color: #436c20; /* darkgreen color for medal */
         font-weight: bold;
     }
     
     /* Caption Styling */
     h6 em {
         display: block;
         font-size: 14px;
         font-style: italic;
         margin-top: 10px;
     }
        
    </style>
</head>
<body>
           <div class="header-container">
                <img src="assets/arab_logo.png" alt="logo" class="logo">
                <h1 class="title">ARB</h1>
            </div>
            <h1 class="title2">A Comprehensive Arabic Reasoning Benchmark</h1>
            <!-- Authors -->
            <div class="authors">
                <p>
                    Sara Ghaboural<sup style="color:#3399FF;">1*</sup>, 
                    Ketan More<sup style="color:#3399FF;">1*</sup>, 
                    Wafa Alghallabi<sup style="color:#3399FF;">1</sup>, 
                    Omkar Thawakar<sup style="color:#3399FF;">1</sup>, 
                    Jorma Laaksonen<sup style="color:#FF66B3">2</sup>, <br>
                    Hisham Cholakkal<sup style="color:#3399FF;">1, </sup>
                    Salman Khan<sup style="color:#3399FF;">1, </sup><sup style="color:#4CB5AE;">3</sup>,
                    Rao M. Anwer<sup style="color:#3399FF;">1, </sup><sup style="color:#FF66B3;">2</sup>,
                </p>
                <p>
                    <sup style="color:#3399FF;">1</sup>Mohamed bin Zayed University of AI, 
                     <sup style="color:#FF66B3;">2</sup>Aalto University
                    <sup style="color:#4CB5AE;">3</sup>Australian National University,
                </p>
            </div>

            <div class="publication-links has-text-centered">
                <span class="link-block">
                    <a href="https://github.com/mbzuai-oryx/ARB/blob/main/assets/ARB_22_05_2025.pdf" class="external-link button">
                        <img src="assets/pdf.png" alt="PDF">
                        <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://arxiv.org/abs/2505.17021" class="external-link button">
                        <img src="assets/arxiv_1.png" alt="arXiv">
                        <span>arXiv</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/mbzuai-oryx/ARB." class="external-link button">
                        <img src="assets/git_1.png" alt="GitHub">
                        <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://huggingface.co/datasets/MBZUAI/ARB" class="external-link button">
                        <span class="icon" style="font-size:18px">ü§ó</span>
                        <span>Dataset</span>
                        </a>
                    </span>
            </div>
    
         
            <!-- Introductory Figure -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark </h1>
                <img src="assets/arb_sample_intro.png" alt="ARB Overview" style="width:80%; height:auto;">
                <p class="fig-caption">
                    <p><h6><em>
        <strong>Figure 1. ARB Scope and Diversity </strong>ARB comprises a wide array of multimodal reasoning samples, each combining a visual input with an Arabic question and detailed step-by-step reasoning with actions taken by step. The dataset spans 11 distinct domains, including visual reasoning, OCR and document understanding, chart and diagram interpretation, mathematical and logical inference, scientific and medical analysis, cultural and historical interpretation, remote sensing, agricultural image analysis, and complex visual perception‚Äîcapturing the linguistic richness, cultural depth, and cross-domain complexity essential for evaluating reasoning in Arabic.
              </em> </h6>
        </div>

 
            <!-- Abstract Section -->
            <div class="abstract-section section-spacing">
                <h2 class="title2">Overview</h2>
                <p>
The Comprehensive Arabic Multimodal Reasoning Benchmark (ARB) is the first benchmark designed to evaluate step-by-step reasoning in Arabic across both textual and visual modalities. Covering 11 diverse domains, ARB includes 1,356 multimodal samples and 5,119 curated reasoning steps. It provides a structured framework to assess the capabilities of open- and closed-source large multimodal models (LMMs), addressing gaps in coherence, cultural grounding, and faithfulness often overlooked in benchmarks focused solely on English.                </p>
            </div>
    
            <!-- Pipeline Figure -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB Construction Pipeline</h1>
                <img src="assets/arb_pipeline.png" alt="ARBpipeline">
                <p class="fig-caption">
                     <p><h6><em>
                  <strong>Figure 2. ARB Pipeline </strong>The figure illustrates the ARB pipeline for evaluating Arabic multimodal reasoning in LMMs. It begins with data collection across 11 domains‚Äîsuch as medical imaging, historical interpretation, visual reasoning, and agriculture‚Äîsourced from curated datasets (e.g., VRC-Bench, CAMEL-Bench), synthetic content, tool-augmented outputs, and web scraping. Data is generated across five categories: English reasoning chains, Arabic Q\&A, English captions, synthetic samples, and tool-enhanced content. Reasoning steps are refined via human-in-the-loop feedback and filtered for logical consistency and cultural alignment. The benchmark supports fine-grained evaluation of open- and closed-source models on Arabic step-by-step reasoning.
                        </em> </h6> </p>
                 </div>
              
                
        <!-- Collection Figureg -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB Data Collection</h1>
                <img src="assets/arb_collection.png" alt="ARBcollect" style="width:80%; height:auto;">
<!--                  <p class="fig-caption"> -->
                    <p><h6><em>
                   <strong>Figure 3.ARB Data Collection </strong>Overview of the ARB Data Collection, Generation and Verification Framework.} The ARB benchmark is constructed from five primary data sources: (1) English reasoning benchmarks, (2) Arabic question‚Äìanswer benchmarks, (3) English-captioned datasets, (4) Synthetic data, and (5) Tool-augmented data. All data undergoes iterative refinement through human-in-the-loop feedback and validation by native Arabic speakers to ensure cultural and linguistic fidelity.  
                </em> </h6> </p> 
            </div>     
    
 <!-- Collection Distribution  -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB Data Distribution</h1>
                <img src="assets/arb_dist.png" alt="ARBdistt" style="width:40%; height:auto;">
<!--                <p class="fig-caption"> -->
               <p><h6><em>
                   <strong>Figure 4. A Domain Distribution in ARB.</strong> The figure shows the distribution of ARB samples across 11 domains. Math & Logic (41%) and Charts, Diagrams, & Tables (24%) dominate, reflecting the dataset‚Äôs emphasis on structured reasoning. Other domains, including Social & Cultural, Scientific, and Medical, add thematic diversity.
                </em> </h6></p>
             </div>
               
     <div class="abstract-section section-spacing">
             <h2 class="title2">Quantitative Evaluation and Results</h2>
    </div>
         <!-- Qualitative Errors in Closed-Source Models -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">Evaluation Metric</h1>
                <img src="assets/arb_rubric.png" alt="ARBeval" style="width:90%; height:auto;">
                <p class="fig-caption">
                    
                <h6><em>
                   <strong>Figure 5. Arabic Reasoning Evaluation Metrics.</strong>  We assess step-by-step reasoning using five core Arabic-specific dimensions: Faithfulness (At-Tat¬Øabuq), Informativeness (Al-Ithr¬Øa‚Äô Al-Ma‚Äôl¬Øum¬Øat¬Øƒ±), Coherence (At-Taw¬Øafuq), Commonsense (Al-Mantiq Al-‚ÄôA¬Ømm), and Reasoning Alignment (At-Tawa¬Øfuq Al-Istidla¬Øl¬Øƒ±). Auxiliary checks cover hallucinations, redundancy, semantic gaps, and missing steps. Metrics are defined at the step and/or token level. The full evaluation rubric is provided in English in Appendix E.
                </em></h6> </p>
        </div>

<br>
<br>
                

<div style="width:80%; margin: 0 auto;">
  <table>
    <thead>
      <tr style="background-color: #c5d9d9; color: white;">
        <th>Model</th>
        <th>GPT-4o</th>
        <th>GPT-4o-min</th>
        <th>GPT-4.1</th>
        <th>o4-mini</th>
        <th>Gemini 1.5 Pro</th>
        <th>Gemini 2.0 Flash</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Final Answer (%)</td>
        <td><b>60.22üèÖ</b></td>
        <td>52.22</td>    
        <td>59.43</td>
        <td>58.93</td>
        <td>56.70</td>
        <td>57.80</td>
      </tr>
      <tr>
        <td>Reasoning Steps (%)</td>
        <td>64.29</td>     
        <td>61.02</td>
        <td>80.41</td>
        <td><b>80.75üèÖ</b></td>
        <td>64.34</td>
        <td>64.09</td>
      </tr>
    </tbody>
  </table>

  <h6 style="text-align: center; font-style: italic; margin-top: 10px;">
    <em>
      <strong>Table 1: Stepwise Evaluation Using LLM-as-Judge ‚Äì Closed-Source Models.</strong>
      Comparison of closed-weight models based on final answer accuracy and aggregated quality scores of reasoning steps, using our LLM-as-Judge framework with Arabic prompts and evaluation metrics. The evaluation follows a reference-based, attribute-level protocol for assessing reasoning quality.
    </em>
  </h6>
<br>
<table>
    <thead>
        <tr style="background-color: #d5e6f4; color: black;">
            <th style="background-color: #d5e6f4; color: black;"> Model</th>
            <th style="background-color: #d5e6f4; color: black;"> Qwen2.5VL-7b</th>
            <th style="background-color: #d5e6f4; color: black;"> Llama-3.2-11B-Vis-Inst.</th>
            <th style="background-color: #d5e6f4; color: black;"> AIN</th>
            <th style="background-color: #d5e6f4; color: black;"> Llama-4-Scout-17Bx16E</th>
            <th style="background-color: #d5e6f4; color: black;"> Aya-Vision-8B</th>
            <th style="background-color: #d5e6f4; color: black;"> InternVl3-8B</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Final Answer (%)</td>
            <td>37.02</td>     
            <td>25.58</td>    
            <td>27.35</td>
            <td><b>48.52üèÖ</b></td>
            <td>28.81</td>
            <td>31.04</td>
        </tr>
        <tr>
            <td>Reasoning Steps (%)</td>
            <td>64.03</td>          
            <td>53.20</td>
            <td>52.77</td>
            <td><b>77.70üèÖ</b></td>
            <td>63.64</td>
            <td>54.50</td>
        </tr>
    </tbody>
</table>
<h6 style="text-align: center; font-style: italic; margin-top: 10px;">
    <em>
      <strong>Table2: Stepwise Evaluation Using LLM-as-Judge- Open-Source Modles.</strong>
 Comparison of closed-weight models based on final answer accuracy and aggregated quality scores of reasoning steps, using our LLM-as-Judge framework with Arabic prompts and evaluation metrics. The evaluation follows a reference-based, attribute-level protocol for assessing reasoning quality.</em>    </em>
</h6>
</div>


    
   <div class="abstract-section section-spacing">
                       <h2 class="title2">Qulitative Examples</h2>
                   </div>
                
  <!-- Qualitative Errors in Closed-Source Models -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">Qualitative Errors in Closed-Source Models</h1>
                <img src="assets/closed_ex.png" alt="ARBclosed" style="width:80%; height:auto;">
<!--                 <p class="fig-caption"> -->
                 <p><h6><em>
                   <strong>Figure 6. Qualitative Errors in Closed-Source Models. </strong> This figure highlights reasoning failures by closed-source LMMs across various Arabic multimodal tasks. Common issues include incorrect numerical comparisons, invalid assumptions, misinterpreted constraints, and logically inconsistent step sequences. These errors often lead to incorrect conclusions despite the appearance of structured reasoning, underscoring the limitations of current closed models when operating in Arabic.
                 </em> </h6>  </p> 
            </div>
    
  <!-- Qualitative Errors in Open-Source Models -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">Qualitative Errors in Open-Source Models</h1>
                <img src="assets/open_ex.png" alt="ARBopen" style="width:80%; height:auto;">
<!--                 <p class="fig-caption"> -->
                  <p><h6><em>
                    <strong>Figure 7. Qualitative Errors in Open-Source Models. </strong>This figure showcases common reasoning flaws in open-source LMMs across diverse Arabic multimodal tasks. Errors include incomplete reasoning steps, inconsistent logic, and hallucinated interpretations not grounded in the input. These issues often result in incorrect answers or unreliable outputs, reflecting the challenges open models face in structured Arabic reasoning.
                   </em> </h6>  </p> 
              </div> 
    
    <!-- Cross-Lingual Reasoning Comparison  -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3"> Cross-Lingual Reasoning Comparison </h1>
                <img src="assets/comparison.png" alt="ARBcross" style="width:90%; height:auto;">
                <p class="fig-caption">
                    <p><h6><em>
                   <strong>Figure 8. Cross-Lingual Reasoning Comparison (Arabic vs. English).  </strong>This figure compares LMMs (GPT-4o) reasoning steps in Arabic and English for the same visual task. In the Arabic version, the model misinterprets structural constraints, yellow highlights incorrect assumptions about equal line counts across boxes, green emphasizes miscounted lines within the boxes, and cyan marks an irrelevant search for a box with exactly 4 lines. These reasoning flaws lead to the wrong answer (C). In contrast, the English reasoning is structured, accurate, and constraint-aware, correctly identifying the answer (A), highlighting the performance gap in Arabic.
                            </em> </h6>  </p>   
             </div>   

   <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre class="bibtex-block"><code>@misc{ghaboura2025arbcomprehensivearabicmultimodal,
  title={ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark}, 
  author={Sara Ghaboura and Ketan More and Wafa Alghallabi and Omkar Thawakar and Jorma Laaksonen and Hisham Cholakkal and Salman Khan and Rao Muhammad Anwer},
  year={2025},
  eprint={2505.17021},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2505.17021}, 
}</code></pre>
  </div>
</section>


<section>
  <div class="section" id="org-banners" style="display:flex; justify-content:center;">
    <a href="https://github.com/mbzuai-oryx" target="_blank" rel="external">
      <img class="org-banner ival-logo" src="assets/IVAL_logo.png">
    </a>
    <a href="https://github.com/mbzuai-oryx" target="_blank" class="ext-link">
      <img class="org-banner oryx-logo" src="assets/Oryx_logo.jpeg">
    </a>
    <a href="https://mbzuai.ac.ae/" target="_blank" rel="external">
      <img class="org-banner mbzuai-logo" src="assets/MBZUAI_logo.png">
    </a>
  </div>
</section>

    
    <footer class="footer">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a
                            href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                            Commons Attribution-ShareAlike 4.0 International License</a>.
                     </p>
                </div>
            </div>
        </div>
    </footer>
  </body>
</html>
